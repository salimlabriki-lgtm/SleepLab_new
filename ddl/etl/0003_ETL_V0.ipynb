{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfc4c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec .venv_sleeplab (Python 3.13.5) nécessitent le package ipykernel.\n",
      "\u001b[1;31mInstallez « ipykernel » dans l’environnement Python. \n",
      "\u001b[1;31mCommande : « /workspaces/SleepLab_new/.venv_sleeplab/bin/python -m pip install ipykernel -U --force-reinstall »"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SleepLab ETL V0 - S0001\n",
    "# PANDORE Metadata + Hypnogram CSV + EDF PSG\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "import mne\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. Logging\n",
    "# ============================================================\n",
    "\n",
    "logger = logging.getLogger(\"sleeplab_etl_s0001\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    )\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Configuration chemins & paramètres\n",
    "# ============================================================\n",
    "\n",
    "# On fixe explicitement la racine du projet (Codespace)\n",
    "BASE_DIR = Path(\"/workspaces/SleepLab_new\").resolve()\n",
    "\n",
    "# Dossier contenant les fichiers bruts PANDORE\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\" / \"PANDORE_SOURCE\"\n",
    "\n",
    "# Fichiers pour la session S0001\n",
    "METADATA_CSV = RAW_DIR / \"PANDORE_SAS_DATASET_Metadata.csv\"\n",
    "HYPNO_CSV    = RAW_DIR / \"PANDORE_SAS_DATASET_S0001_Hypnogram.csv\"\n",
    "EDF_FILE     = RAW_DIR / \"PANDORE_SAS_DATASET_S0001_PSG_1.edf\"\n",
    "\n",
    "# Paramètres métier\n",
    "EPOCH_LENGTH_SEC = 30              # Durée d'une epoch en secondes (V0 : pas encore utilisé pour regridding)\n",
    "DEFAULT_TZ       = \"Europe/Brussels\"\n",
    "\n",
    "# Regex pour détecter un \"time-only\" du type HH:MM ou HH:MM:SS\n",
    "HOUR_ONLY_RE = re.compile(r\"^\\s*\\d{1,2}:\\d{2}(:\\d{2})?\\s*$\")\n",
    "\n",
    "# Types de canaux connus dans ref_channel_type\n",
    "REF_CHANNEL_TYPES = {\"EEG\", \"EOG\", \"EMG\", \"OXIMETER\", \"AUDIO\", \"OTHER\"}\n",
    "\n",
    "# Configuration Postgres : variable d'environnement SLEEPLAB_DB_DSN\n",
    "DB_DSN = os.getenv(\n",
    "    \"SLEEPLAB_DB_DSN\",\n",
    "    \"postgresql://sleeplab:sleeplab@postgres:5432/sleeplab\",\n",
    ")\n",
    "logger.info(\"DB_DSN utilisé : %s\", DB_DSN)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Connexion Postgres & helpers\n",
    "# ============================================================\n",
    "\n",
    "conn = psycopg2.connect(DB_DSN)\n",
    "\n",
    "# On fixe le search_path sur le schéma 'sleeplab'\n",
    "with conn.cursor() as _cur:\n",
    "    _cur.execute(\"SET search_path TO sleeplab\")\n",
    "conn.commit()\n",
    "logger.info(\"Connexion Postgres OK, search_path fixé sur 'sleeplab'\")\n",
    "\n",
    "\n",
    "def db_cursor(dict_cursor: bool = False):\n",
    "    \"\"\"\n",
    "    Helper pour obtenir un curseur Postgres.\n",
    "\n",
    "    - dict_cursor=True : lignes renvoyées sous forme de dict (RealDictCursor).\n",
    "    - dict_cursor=False : tuples classiques.\n",
    "    \"\"\"\n",
    "    cursor_factory = RealDictCursor if dict_cursor else None\n",
    "    return conn.cursor(cursor_factory=cursor_factory)\n",
    "\n",
    "\n",
    "def require_file(path: Path, label: str):\n",
    "    \"\"\"\n",
    "    Vérifie qu'un fichier existe et est bien un fichier.\n",
    "    Logue explicitement l'erreur si absent.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        logger.error(\"[FICHIER MANQUANT] %s : %s\", label, path)\n",
    "        raise FileNotFoundError(f\"{label} non trouvé : {path}\")\n",
    "    if not path.is_file():\n",
    "        logger.error(\"[CHEMIN NON FICHIER] %s : %s\", label, path)\n",
    "        raise FileNotFoundError(f\"{label} n'est pas un fichier : {path}\")\n",
    "    logger.info(\"[OK] %s trouvé : %s\", label, path)\n",
    "\n",
    "\n",
    "def sanitize_for_json(value):\n",
    "    \"\"\"\n",
    "    Nettoie une valeur pour qu'elle soit sérialisable en JSON standard :\n",
    "\n",
    "    - NaN / NaT / valeurs pandas nulles -> None\n",
    "    - datetime / Timestamp -> string isoformat()\n",
    "    - 'nan' / 'NaN' / 'NaT' / '' -> None\n",
    "    - Types numpy scalaires -> valeur Python native\n",
    "    - numpy.ndarray -> list\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(value, float) and math.isnan(value):\n",
    "        return None\n",
    "\n",
    "    if isinstance(value, (pd.Timestamp, dt.datetime)):\n",
    "        return value.isoformat()\n",
    "\n",
    "    if isinstance(value, str) and value.strip().lower() in (\"nan\", \"nat\", \"\"):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        import numpy as np\n",
    "\n",
    "        if isinstance(value, (np.generic,)):\n",
    "            return value.item()\n",
    "\n",
    "        if isinstance(value, np.ndarray):\n",
    "            return value.tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def normalize_channel_type_for_ref(channel_type_code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    S'assure que le channel_type_code est compatible avec ref_channel_type.\n",
    "    Si le code n'est pas connu (ex: 'RESP'), on le remappe vers 'OTHER'.\n",
    "    \"\"\"\n",
    "    if channel_type_code is None:\n",
    "        return None\n",
    "\n",
    "    code = channel_type_code.upper()\n",
    "    if code in REF_CHANNEL_TYPES:\n",
    "        return code\n",
    "\n",
    "    logger.warning(\n",
    "        \"channel_type_code '%s' non présent dans ref_channel_type, remappé à 'OTHER'\",\n",
    "        code,\n",
    "    )\n",
    "    return \"OTHER\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Chargement des données sources\n",
    "# ============================================================\n",
    "\n",
    "def load_metadata_dataframe() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Charge le metadata PANDORE (SAS export) dans un DataFrame.\n",
    "    On laisse pandas détecter le séparateur (sep=None, engine='python').\n",
    "    \"\"\"\n",
    "    require_file(METADATA_CSV, \"PANDORE metadata CSV\")\n",
    "    logger.info(\"Lecture metadata PANDORE : %s\", METADATA_CSV)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        METADATA_CSV,\n",
    "        sep=None,\n",
    "        engine=\"python\",\n",
    "        dtype=str,\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Metadata chargé : %d lignes, %d colonnes. Colonnes=%s\",\n",
    "        len(df),\n",
    "        len(df.columns),\n",
    "        list(df.columns),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_metadata_row_for_s0001(df_meta: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Sélectionne la ligne de metadata correspondant à S0001\n",
    "    en se basant sur la colonne Folder_id (ou toute colonne contenant 'folder').\n",
    "    \"\"\"\n",
    "    folder_id_value = \"PANDORE_SAS_DATASET_S0001\"\n",
    "\n",
    "    folder_col = None\n",
    "    for col in df_meta.columns:\n",
    "        if \"folder\" in col.lower():\n",
    "            folder_col = col\n",
    "            break\n",
    "\n",
    "    if folder_col is None:\n",
    "        logger.error(\n",
    "            \"Impossible de trouver une colonne de type Folder_id. Colonnes disponibles : %s\",\n",
    "            list(df_meta.columns),\n",
    "        )\n",
    "        raise KeyError(\"Colonne Folder_id introuvable dans le metadata PANDORE\")\n",
    "\n",
    "    subset = df_meta[df_meta[folder_col] == folder_id_value]\n",
    "\n",
    "    if subset.empty:\n",
    "        logger.error(\n",
    "            \"Aucune ligne metadata pour Folder_id=%s (colonne=%s)\",\n",
    "            folder_id_value,\n",
    "            folder_col,\n",
    "        )\n",
    "        raise KeyError(f\"Aucune ligne metadata pour Folder_id={folder_id_value}\")\n",
    "\n",
    "    if len(subset) > 1:\n",
    "        logger.warning(\n",
    "            \"%d lignes metadata pour Folder_id=%s ; la première sera utilisée.\",\n",
    "            len(subset),\n",
    "            folder_id_value,\n",
    "        )\n",
    "\n",
    "    row = subset.iloc[0]\n",
    "    logger.info(\n",
    "        \"Ligne metadata sélectionnée pour %s (index=%s). Exemple de valeurs: %s\",\n",
    "        folder_id_value,\n",
    "        row.name,\n",
    "        row.to_dict(),\n",
    "    )\n",
    "    return row\n",
    "\n",
    "\n",
    "def load_hypnogram_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Charge l'hypnogramme PANDORE pour S0001 dans un DataFrame,\n",
    "    en s'assurant que les colonnes begin/end/event sont bien parsées,\n",
    "    même s'il y a un BOM ou des espaces/casse différents.\n",
    "    \"\"\"\n",
    "    require_file(HYPNO_CSV, \"PANDORE hypnogram CSV\")\n",
    "    logger.info(\"Lecture hypnogramme : %s\", HYPNO_CSV)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        HYPNO_CSV,\n",
    "        sep=None,\n",
    "        engine=\"python\",\n",
    "        dtype=str,\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Hypnogramme chargé : %d lignes, %d colonnes. Colonnes brutes=%s\",\n",
    "        len(df),\n",
    "        len(df.columns),\n",
    "        list(df.columns),\n",
    "    )\n",
    "\n",
    "    # Normalisation des noms de colonnes (BOM, espaces, casse)\n",
    "    col_map = {}\n",
    "    for c in df.columns:\n",
    "        norm = c.strip().lstrip(\"\\ufeff\").lower()\n",
    "        if norm == \"begin\":\n",
    "            col_map[c] = \"begin\"\n",
    "        elif norm == \"end\":\n",
    "            col_map[c] = \"end\"\n",
    "        elif norm == \"event\":\n",
    "            col_map[c] = \"event\"\n",
    "\n",
    "    if col_map:\n",
    "        df = df.rename(columns=col_map)\n",
    "\n",
    "    missing = []\n",
    "    for col in (\"begin\", \"end\", \"event\"):\n",
    "        if col not in df.columns:\n",
    "            missing.append(col)\n",
    "\n",
    "    if missing:\n",
    "        logger.error(\n",
    "            \"Colonnes obligatoires manquantes dans l'hypnogramme : %s. Colonnes présentes=%s\",\n",
    "            missing,\n",
    "            list(df.columns),\n",
    "        )\n",
    "        raise KeyError(f\"Colonnes manquantes dans l'hypnogramme PANDORE : {missing}\")\n",
    "\n",
    "    # Conversion begin/end en Timestamp UTC\n",
    "    df[\"begin\"] = pd.to_datetime(df[\"begin\"], errors=\"coerce\", utc=True)\n",
    "    df[\"end\"]   = pd.to_datetime(df[\"end\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    nb_null_begin = df[\"begin\"].isna().sum()\n",
    "    nb_null_end   = df[\"end\"].isna().sum()\n",
    "    if nb_null_begin or nb_null_end:\n",
    "        logger.warning(\n",
    "            \"Hypnogramme : begin NaN=%d, end NaN=%d\",\n",
    "            nb_null_begin,\n",
    "            nb_null_end,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_edf() -> mne.io.BaseRaw:\n",
    "    \"\"\"\n",
    "    Charge l'EDF PSG S0001 via mne.\n",
    "    \"\"\"\n",
    "    require_file(EDF_FILE, \"EDF PSG S0001\")\n",
    "    logger.info(\"Chargement EDF via mne : %s\", EDF_FILE)\n",
    "    raw = mne.io.read_raw_edf(str(EDF_FILE), preload=False, verbose=False)\n",
    "\n",
    "    info = raw.info\n",
    "    logger.info(\n",
    "        \"EDF chargé : nchan=%d, sfreq=%.3f, meas_date=%s, n_times=%d\",\n",
    "        info[\"nchan\"],\n",
    "        info[\"sfreq\"],\n",
    "        info[\"meas_date\"],\n",
    "        raw.n_times,\n",
    "    )\n",
    "    return raw\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. ETL subject\n",
    "# ============================================================\n",
    "\n",
    "def etl_subject_from_metadata(row_meta: pd.Series) -> Tuple[uuid.UUID, int]:\n",
    "    \"\"\"\n",
    "    Crée (ou récupère) le sujet 'S0001' dans la table subject.\n",
    "    Pour cette V0 : on ne lit pas encore les infos patient dans l'EDF/metadata.\n",
    "    \"\"\"\n",
    "    subject_code = \"S0001\"\n",
    "    logger.info(\"ETL subject pour subject_code=%s\", subject_code)\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"SELECT subject_id FROM subject WHERE subject_code = %s\",\n",
    "            (subject_code,),\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        if row:\n",
    "            subject_id = row[0]\n",
    "            logger.info(\n",
    "                \"Sujet déjà présent, subject_id=%s → aucune nouvelle ligne.\",\n",
    "                subject_id,\n",
    "            )\n",
    "            return subject_id, 0\n",
    "\n",
    "    subject_id = uuid.uuid4()\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO subject (\n",
    "                subject_id, subject_code, source_system,\n",
    "                source_subject_id, birth_year, sex,\n",
    "                height_cm, weight_kg, bmi, notes\n",
    "            )\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(subject_id),\n",
    "                subject_code,\n",
    "                \"PANDORE\",\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "            ),\n",
    "        )\n",
    "    logger.info(\n",
    "        \"Nouveau sujet inséré, subject_id=%s → 1 ligne insérée dans subject\",\n",
    "        subject_id,\n",
    "    )\n",
    "    return subject_id, 1\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. ETL study_session (avec correction du bug de date)\n",
    "# ============================================================\n",
    "\n",
    "def parse_session_start_from_metadata(\n",
    "    row_meta: pd.Series,\n",
    ") -> Optional[Union[pd.Timestamp, str]]:\n",
    "    \"\"\"\n",
    "    Essaie de récupérer start_time depuis le metadata.\n",
    "\n",
    "    - Si on détecte une heure seule (HH:MM ou HH:MM:SS) → on renvoie la chaîne.\n",
    "    - Si on parvient à parser un datetime complet → on renvoie un Timestamp UTC.\n",
    "    - Sinon → None.\n",
    "    \"\"\"\n",
    "    if \"start_time\" not in row_meta.index:\n",
    "        logger.warning(\n",
    "            \"Colonne start_time absente du metadata → session_start_utc sera déduite de l'hypnogramme ou NULL\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    raw_val = row_meta[\"start_time\"]\n",
    "    if pd.isna(raw_val) or str(raw_val).strip() == \"\":\n",
    "        logger.warning(\n",
    "            \"start_time vide dans le metadata → session_start_utc sera déduite de l'hypnogramme ou NULL\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    txt = str(raw_val).strip()\n",
    "\n",
    "    if HOUR_ONLY_RE.match(txt):\n",
    "        logger.info(\n",
    "            \"start_time '%s' détecté comme heure (sans date) → \"\n",
    "            \"il sera combiné avec la date du premier 'begin' de l'hypnogramme.\",\n",
    "            txt,\n",
    "        )\n",
    "        return txt\n",
    "\n",
    "    try:\n",
    "        ts = pd.to_datetime(txt, utc=True)\n",
    "        logger.info(\n",
    "            \"start_time '%s' parsé comme datetime complet : %s\",\n",
    "            txt,\n",
    "            ts,\n",
    "        )\n",
    "        return ts\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            \"Impossible de parser start_time '%s' comme datetime complet : %s → sera ignoré.\",\n",
    "            txt,\n",
    "            e,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_session_start_utc(\n",
    "    row_meta: pd.Series,\n",
    "    df_hypno: pd.DataFrame,\n",
    ") -> Optional[pd.Timestamp]:\n",
    "    \"\"\"\n",
    "    Construit un Timestamp UTC pour session_start_utc à partir :\n",
    "      - de start_time (metadata), si possible\n",
    "      - sinon de la première date 'begin' de l'hypnogramme\n",
    "      - ou combinaison date(begin) + heure(start_time)\n",
    "    \"\"\"\n",
    "    start_meta = parse_session_start_from_metadata(row_meta)\n",
    "\n",
    "    if isinstance(start_meta, pd.Timestamp):\n",
    "        logger.info(\n",
    "            \"session_start_utc pris directement du metadata (datetime complet) : %s\",\n",
    "            start_meta,\n",
    "        )\n",
    "        return start_meta\n",
    "\n",
    "    if df_hypno is None or df_hypno.empty or \"begin\" not in df_hypno.columns:\n",
    "        logger.warning(\n",
    "            \"Pas d'hypnogramme disponible pour inférer la date de début, session_start_utc sera NULL.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    first_begin = df_hypno[\"begin\"].dropna().min()\n",
    "    if not isinstance(first_begin, pd.Timestamp):\n",
    "        logger.warning(\n",
    "            \"Impossible de convertir 'begin' en Timestamp, session_start_utc sera NULL.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    if start_meta is None:\n",
    "        logger.info(\n",
    "            \"Aucun start_time exploitable, session_start_utc pris depuis le premier 'begin' de l'hypnogramme : %s\",\n",
    "            first_begin,\n",
    "        )\n",
    "        return first_begin\n",
    "\n",
    "    if isinstance(start_meta, str) and HOUR_ONLY_RE.match(start_meta.strip()):\n",
    "        date_part = first_begin.date().isoformat()\n",
    "        time_part = start_meta.strip()\n",
    "        try:\n",
    "            combined = pd.to_datetime(f\"{date_part} {time_part}\", utc=True)\n",
    "            logger.info(\n",
    "                \"session_start_utc combiné : date hypnogramme (%s) + heure start_time (%s) → %s\",\n",
    "                date_part,\n",
    "                time_part,\n",
    "                combined,\n",
    "            )\n",
    "            return combined\n",
    "        except Exception as e:\n",
    "            logger.warning(\n",
    "                \"Impossible de combiner date hypnogramme + start_time (%s) : %s ; on garde begin=%s\",\n",
    "                time_part,\n",
    "                e,\n",
    "                first_begin,\n",
    "            )\n",
    "            return first_begin\n",
    "\n",
    "    logger.warning(\n",
    "        \"start_meta de type inattendu (%r), session_start_utc pris depuis le premier 'begin' : %s\",\n",
    "        start_meta,\n",
    "        first_begin,\n",
    "    )\n",
    "    return first_begin\n",
    "\n",
    "\n",
    "def etl_study_session_from_metadata(\n",
    "    row_meta: pd.Series,\n",
    "    subject_id: uuid.UUID,\n",
    "    df_hypno: pd.DataFrame,\n",
    ") -> Tuple[uuid.UUID, int, Optional[pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Crée (ou récupère) la study_session associée à S0001.\n",
    "    \"\"\"\n",
    "    folder_id = None\n",
    "    for col in row_meta.index:\n",
    "        if \"folder\" in col.lower():\n",
    "            folder_id = row_meta[col]\n",
    "            break\n",
    "    if not folder_id:\n",
    "        folder_id = \"PANDORE_SAS_DATASET_S0001\"\n",
    "        logger.warning(\n",
    "            \"Aucune colonne Folder_id dans la ligne metadata → session_code par défaut = %s\",\n",
    "            folder_id,\n",
    "        )\n",
    "\n",
    "    session_code = str(folder_id)\n",
    "    session_start_utc = build_session_start_utc(row_meta, df_hypno)\n",
    "\n",
    "    protocol_code = None\n",
    "    for col in row_meta.index:\n",
    "        if \"study_type\" in col.lower() or col.lower() == \"type\":\n",
    "            protocol_code = row_meta[col]\n",
    "            break\n",
    "\n",
    "    logger.info(\n",
    "        \"ETL study_session pour session_code=%s, protocol_code=%s\",\n",
    "        session_code,\n",
    "        protocol_code,\n",
    "    )\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT session_id\n",
    "            FROM study_session\n",
    "            WHERE session_code = %s AND subject_id = %s\n",
    "            \"\"\",\n",
    "            (session_code, str(subject_id)),\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        if row:\n",
    "            session_id = row[0]\n",
    "            logger.info(\n",
    "                \"Session déjà présente, session_id=%s → aucune nouvelle ligne.\",\n",
    "                session_id,\n",
    "            )\n",
    "            return session_id, 0, session_start_utc\n",
    "\n",
    "    session_id = uuid.uuid4()\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO study_session (\n",
    "                session_id, subject_id, session_code,\n",
    "                session_start_utc, session_end_utc,\n",
    "                site_name, protocol_code, scorer_name,\n",
    "                tz_name, comments\n",
    "            )\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(session_id),\n",
    "                str(subject_id),\n",
    "                session_code,\n",
    "                session_start_utc.to_pydatetime()\n",
    "                if isinstance(session_start_utc, pd.Timestamp)\n",
    "                else None,\n",
    "                None,\n",
    "                None,\n",
    "                protocol_code,\n",
    "                None,\n",
    "                DEFAULT_TZ,\n",
    "                None,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    logger.info(\n",
    "        \"Nouvelle session insérée, session_id=%s → 1 ligne insérée dans study_session\",\n",
    "        session_id,\n",
    "    )\n",
    "    return session_id, 1, session_start_utc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. ETL data_file & signal_channel à partir de l'EDF\n",
    "# ============================================================\n",
    "\n",
    "def etl_data_file_from_edf(\n",
    "    raw: mne.io.BaseRaw,\n",
    "    session_id: uuid.UUID,\n",
    ") -> Tuple[uuid.UUID, int]:\n",
    "    \"\"\"\n",
    "    Crée une entrée data_file à partir de l'EDF.\n",
    "    \"\"\"\n",
    "    file_id = uuid.uuid4()\n",
    "    info = raw.info\n",
    "\n",
    "    sfreq = float(info[\"sfreq\"] or 0.0)\n",
    "    n_times = int(raw.n_times)\n",
    "    duration_sec = int(round(n_times / sfreq)) if sfreq > 0 else None\n",
    "    byte_size = EDF_FILE.stat().st_size\n",
    "\n",
    "    logger.info(\n",
    "        \"Préparation data_file EDF : size=%d bytes, duration=%s sec, nchan=%d\",\n",
    "        byte_size,\n",
    "        duration_sec,\n",
    "        info[\"nchan\"],\n",
    "    )\n",
    "\n",
    "    manufacturer = None\n",
    "    device_serial = None\n",
    "    device_info = info.get(\"device_info\") or {}\n",
    "    if isinstance(device_info, dict):\n",
    "        manufacturer = device_info.get(\"manufacturer\") or device_info.get(\"name\")\n",
    "        device_serial = device_info.get(\"serial\")\n",
    "\n",
    "    manufacturer = manufacturer or None\n",
    "\n",
    "    device_serial_hash = None\n",
    "    if device_serial:\n",
    "        device_serial_hash = str(\n",
    "            uuid.uuid5(uuid.NAMESPACE_DNS, str(device_serial))\n",
    "        )\n",
    "\n",
    "    header = {\n",
    "        \"meas_date\": sanitize_for_json(info.get(\"meas_date\")),\n",
    "        \"nchan\": sanitize_for_json(info.get(\"nchan\")),\n",
    "        \"sfreq\": sanitize_for_json(info.get(\"sfreq\")),\n",
    "        \"highpass\": sanitize_for_json(info.get(\"highpass\")),\n",
    "        \"lowpass\": sanitize_for_json(info.get(\"lowpass\")),\n",
    "        \"device_info\": {\n",
    "            k: sanitize_for_json(v) for k, v in (device_info or {}).items()\n",
    "        },\n",
    "    }\n",
    "    raw_header_json_str = json.dumps(header, allow_nan=False)\n",
    "\n",
    "    recorded_start_utc = None\n",
    "    meas_date = info.get(\"meas_date\")\n",
    "    if isinstance(meas_date, pd.Timestamp):\n",
    "        recorded_start_utc = meas_date.isoformat()\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO data_file (\n",
    "                file_id, session_id,\n",
    "                file_type_code, source_path, file_name,\n",
    "                sha256_hex, byte_size,\n",
    "                recorded_start_utc, recorded_duration_sec,\n",
    "                n_channels, manufacturer, device_serial_hash,\n",
    "                ingest_ts, raw_header_json\n",
    "            )\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(file_id),\n",
    "                str(session_id),\n",
    "                \"EDF\",\n",
    "                str(EDF_FILE),\n",
    "                EDF_FILE.name,\n",
    "                None,\n",
    "                byte_size,\n",
    "                recorded_start_utc,\n",
    "                duration_sec,\n",
    "                info[\"nchan\"],\n",
    "                manufacturer,\n",
    "                device_serial_hash,\n",
    "                raw_header_json_str,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    logger.info(\n",
    "        \"Ligne data_file insérée, file_id=%s → 1 ligne insérée dans data_file\",\n",
    "        file_id,\n",
    "    )\n",
    "    return file_id, 1\n",
    "\n",
    "\n",
    "def map_channel_label_to_type(label: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Mapping très simple label EDF → type de canal.\n",
    "    Peut être complété plus tard.\n",
    "    \"\"\"\n",
    "    if not label:\n",
    "        return None\n",
    "\n",
    "    low = label.lower()\n",
    "\n",
    "    if low.startswith(\"eeg\") or low in (\n",
    "        \"c3\",\n",
    "        \"c4\",\n",
    "        \"f3\",\n",
    "        \"f4\",\n",
    "        \"o1\",\n",
    "        \"o2\",\n",
    "        \"c3-m2\",\n",
    "        \"c4-m1\",\n",
    "        \"f3-m2\",\n",
    "        \"f4-m1\",\n",
    "    ):\n",
    "        return \"EEG\"\n",
    "\n",
    "    if \"eog\" in low or low.startswith(\"e1\") or low.startswith(\"e2\"):\n",
    "        return \"EOG\"\n",
    "\n",
    "    if \"chin\" in low or \"emg\" in low:\n",
    "        return \"EMG\"\n",
    "\n",
    "    if \"ox\" in low or \"spo2\" in low:\n",
    "        return \"OXIMETER\"\n",
    "\n",
    "    if \"thorax\" in low or \"abdomen\" in low or \"resp\" in low:\n",
    "        return \"RESP\"\n",
    "\n",
    "    if \"mic\" in low or \"snore\" in low or \"audio\" in low:\n",
    "        return \"AUDIO\"\n",
    "\n",
    "    return \"OTHER\"\n",
    "\n",
    "\n",
    "def etl_signal_channels_from_edf(\n",
    "    raw: mne.io.BaseRaw,\n",
    "    file_id: uuid.UUID,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Remplit la table signal_channel pour tous les canaux de l'EDF.\n",
    "    \"\"\"\n",
    "    info = raw.info\n",
    "    sfreq = float(info[\"sfreq\"] or 0.0)\n",
    "    nchan = info[\"nchan\"]\n",
    "\n",
    "    logger.info(\"ETL signal_channel : nchan=%d, sfreq=%.3f\", nchan, sfreq)\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\"DELETE FROM signal_channel WHERE file_id=%s\", (str(file_id),))\n",
    "        nb_deleted = cur.rowcount\n",
    "    if nb_deleted:\n",
    "        logger.info(\n",
    "            \"%d canal(aux) existant(s) supprimé(s) pour file_id=%s\",\n",
    "            nb_deleted,\n",
    "            file_id,\n",
    "        )\n",
    "\n",
    "    nb_inserted = 0\n",
    "    with db_cursor() as cur:\n",
    "        for idx, label in enumerate(raw.ch_names):\n",
    "            ch_info = info[\"chs\"][idx]\n",
    "            raw_type = map_channel_label_to_type(label)\n",
    "            channel_type_code = normalize_channel_type_for_ref(raw_type)\n",
    "            channel_id = uuid.uuid4()\n",
    "\n",
    "            unit = None\n",
    "            physical_min = None\n",
    "            physical_max = None\n",
    "            digital_min = None\n",
    "            digital_max = None\n",
    "            transducer = None\n",
    "            prefilter = None\n",
    "\n",
    "            if isinstance(ch_info, dict):\n",
    "                unit = ch_info.get(\"unit\")\n",
    "                transducer = ch_info.get(\"ch_name\")\n",
    "                loc = ch_info.get(\"loc\")\n",
    "                if loc is not None:\n",
    "                    prefilter = str(loc)\n",
    "\n",
    "            if raw_type != channel_type_code:\n",
    "                logger.info(\n",
    "                    \"Channel %d ('%s') : type brut='%s' normalisé='%s'\",\n",
    "                    idx,\n",
    "                    label,\n",
    "                    raw_type,\n",
    "                    channel_type_code,\n",
    "                )\n",
    "\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO signal_channel (\n",
    "                    channel_id, file_id,\n",
    "                    channel_index, label,\n",
    "                    channel_type_code, unit,\n",
    "                    sampling_hz, physical_min, physical_max,\n",
    "                    digital_min, digital_max, transducer,\n",
    "                    prefilter, lowcut_hz, highcut_hz, notch_hz\n",
    "                )\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    str(channel_id),\n",
    "                    str(file_id),\n",
    "                    idx,\n",
    "                    label,\n",
    "                    channel_type_code,\n",
    "                    unit,\n",
    "                    sfreq,\n",
    "                    physical_min,\n",
    "                    physical_max,\n",
    "                    digital_min,\n",
    "                    digital_max,\n",
    "                    transducer,\n",
    "                    prefilter,\n",
    "                    info.get(\"highpass\"),\n",
    "                    info.get(\"lowpass\"),\n",
    "                    None,\n",
    "                ),\n",
    "            )\n",
    "            nb_inserted += 1\n",
    "\n",
    "    logger.info(\n",
    "        \"%d ligne(s) insérée(s) dans signal_channel pour file_id=%s\",\n",
    "        nb_inserted,\n",
    "        file_id,\n",
    "    )\n",
    "    return nb_inserted\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Hypnogramme → hypnogram_epoch + event_annotation\n",
    "# ============================================================\n",
    "\n",
    "def map_event_to_sleep_stage(event_str: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Mapping 'event' -> code de stade de sommeil standardisé.\n",
    "\n",
    "    Objectif :\n",
    "    - Accepter le CSV PANDORE qui code les stades en :\n",
    "        AWA, N1, N2, N3, REM\n",
    "    - Garder la compatibilité avec d'éventuels anciens exports\n",
    "      de type 'stage-n1', 'stage-rem', etc.\n",
    "    - Ne rien renvoyer pour les événements non-stade :\n",
    "      arousals, apnées, positions, ...\n",
    "\n",
    "    Retourne :\n",
    "        'W', 'N1', 'N2', 'N3', 'R' ou None\n",
    "    \"\"\"\n",
    "    if not event_str:\n",
    "        return None\n",
    "\n",
    "    s = event_str.strip().lower()\n",
    "\n",
    "    # Nouveau CSV : AWA / N1 / N2 / N3 / REM\n",
    "    if s in (\"awa\", \"wake\", \"awake\", \"w\"):\n",
    "        return \"W\"\n",
    "    if s == \"n1\":\n",
    "        return \"N1\"\n",
    "    if s == \"n2\":\n",
    "        return \"N2\"\n",
    "    if s in (\"n3\", \"n4\"):\n",
    "        return \"N3\"\n",
    "    if s == \"rem\":\n",
    "        return \"R\"\n",
    "\n",
    "    # Anciennes notations : 'stage-n1', 'stage-rem', ...\n",
    "    if \"stage-w\" in s or \"stage w\" in s:\n",
    "        return \"W\"\n",
    "    if \"stage-n1\" in s or (\"n1\" in s and \"stage\" in s):\n",
    "        return \"N1\"\n",
    "    if \"stage-n2\" in s or (\"n2\" in s and \"stage\" in s):\n",
    "        return \"N2\"\n",
    "    if \"stage-n3\" in s or \"stage-n4\" in s or (\"n3\" in s and \"stage\" in s):\n",
    "        return \"N3\"\n",
    "    if \"stage-rem\" in s or (\"rem\" in s and \"stage\" in s):\n",
    "        return \"R\"\n",
    "\n",
    "    # Arousal, apnées, positions, etc. → pas un stade de sommeil\n",
    "    return None\n",
    "\n",
    "\n",
    "def etl_hypnogram_to_epochs_and_events(\n",
    "    df_hypno: pd.DataFrame,\n",
    "    session_id: uuid.UUID,\n",
    "    session_start_utc: Optional[pd.Timestamp],\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Pour l'instant, on considère chaque ligne de l'hypnogramme comme une \"epoch\"\n",
    "    et un événement associé :\n",
    "      - hypnogram_epoch : 1 ligne par évènement, grille V0 (epoch_index = index du DF)\n",
    "      - event_annotation : 1 ligne par évènement, extra_json complet\n",
    "    \"\"\"\n",
    "\n",
    "    if \"begin\" not in df_hypno.columns or \"end\" not in df_hypno.columns:\n",
    "        logger.error(\n",
    "            \"Colonnes 'begin' et/ou 'end' absentes du hypnogramme → ETL hypnogram annulé\"\n",
    "        )\n",
    "        return 0, 0\n",
    "\n",
    "    if \"event\" not in df_hypno.columns:\n",
    "        logger.error(\"Colonne 'event' absente du hypnogramme → ETL hypnogram annulé\")\n",
    "        return 0, 0\n",
    "\n",
    "    logger.info(\n",
    "        \"ETL hypnogram_epoch + event_annotation pour session_id=%s (%d lignes brutes)\",\n",
    "        session_id,\n",
    "        len(df_hypno),\n",
    "    )\n",
    "\n",
    "    nb_epochs_inserted = 0\n",
    "    nb_events_inserted = 0\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"DELETE FROM hypnogram_epoch WHERE session_id=%s\", (str(session_id),)\n",
    "        )\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            DELETE FROM event_annotation\n",
    "            WHERE session_id=%s AND source_name = 'PANDORE_HYPNO'\n",
    "            \"\"\",\n",
    "            (str(session_id),),\n",
    "        )\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        for idx, row in df_hypno.iterrows():\n",
    "            begin_dt = row[\"begin\"]\n",
    "            end_dt = row[\"end\"]\n",
    "\n",
    "            if not isinstance(begin_dt, pd.Timestamp) or not isinstance(\n",
    "                end_dt, pd.Timestamp\n",
    "            ):\n",
    "                logger.debug(\n",
    "                    \"Ligne %s ignorée car begin/end non parsable : %s / %s\",\n",
    "                    idx,\n",
    "                    begin_dt,\n",
    "                    end_dt,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            event_str = row.get(\"event\")\n",
    "            sleep_stage_code = map_event_to_sleep_stage(event_str)\n",
    "\n",
    "            if session_start_utc is not None and isinstance(\n",
    "                session_start_utc, pd.Timestamp\n",
    "            ):\n",
    "                onset_sec = (begin_dt - session_start_utc).total_seconds()\n",
    "            else:\n",
    "                onset_sec = 0.0\n",
    "\n",
    "            duration_sec = (end_dt - begin_dt).total_seconds()\n",
    "\n",
    "            # hypnogram_epoch\n",
    "            epoch_id = uuid.uuid4()\n",
    "            epoch_index = idx  # V0 : 1 ligne d'hypnogram = 1 epoch\n",
    "\n",
    "            scorer_name = row.get(\"scoring_owner\") or row.get(\"scoring_name\")\n",
    "\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO hypnogram_epoch (\n",
    "                    epoch_id, session_id,\n",
    "                    epoch_index, start_sec,\n",
    "                    duration_sec, sleep_stage_code,\n",
    "                    scorer_name\n",
    "                )\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    str(epoch_id),\n",
    "                    str(session_id),\n",
    "                    int(epoch_index),\n",
    "                    int(onset_sec),\n",
    "                    int(duration_sec),\n",
    "                    sleep_stage_code,\n",
    "                    scorer_name,\n",
    "                ),\n",
    "            )\n",
    "            nb_epochs_inserted += 1\n",
    "\n",
    "            # event_annotation\n",
    "            scoring_name = row.get(\"scoring_name\")\n",
    "            scoring_owner = row.get(\"scoring_owner\")\n",
    "            scoring_type = row.get(\"scoring_type\")\n",
    "\n",
    "            extra_raw = {\n",
    "                \"begin\": begin_dt,\n",
    "                \"end\": end_dt,\n",
    "                \"location\": row.get(\"location\"),\n",
    "                \"is_deleted\": row.get(\"is_deleted\"),\n",
    "                \"scoring_name\": scoring_name,\n",
    "                \"scoring_owner\": scoring_owner,\n",
    "                \"scoring_type\": scoring_type,\n",
    "                \"scoring\": row.get(\"scoring\"),\n",
    "            }\n",
    "\n",
    "            extra_clean = {k: sanitize_for_json(v) for k, v in extra_raw.items()}\n",
    "            extra_json_str = json.dumps(extra_clean, allow_nan=False)\n",
    "\n",
    "            event_id = uuid.uuid4()\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO event_annotation (\n",
    "                    event_id, session_id, file_id,\n",
    "                    channel_id, event_type_code,\n",
    "                    event_label, onset_sec, duration_sec,\n",
    "                    severity, source_name, version_tag,\n",
    "                    extra_json\n",
    "                )\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    str(event_id),\n",
    "                    str(session_id),\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    event_str,\n",
    "                    float(onset_sec),\n",
    "                    float(duration_sec),\n",
    "                    None,\n",
    "                    \"PANDORE_HYPNO\",\n",
    "                    scoring_type or scoring_owner or scoring_name,\n",
    "                    extra_json_str,\n",
    "                ),\n",
    "            )\n",
    "            nb_events_inserted += 1\n",
    "\n",
    "    logger.info(\n",
    "        \"%d epoch(s) insérée(s) dans hypnogram_epoch pour session_id=%s\",\n",
    "        nb_epochs_inserted,\n",
    "        session_id,\n",
    "    )\n",
    "    logger.info(\n",
    "        \"%d événement(s) inséré(s) dans event_annotation (source=PANDORE_HYPNO) pour session_id=%s\",\n",
    "        nb_events_inserted,\n",
    "        session_id,\n",
    "    )\n",
    "    return nb_epochs_inserted, nb_events_inserted\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. ETL metric_summary à partir du metadata\n",
    "# ============================================================\n",
    "\n",
    "def etl_metric_summary_from_metadata(\n",
    "    row_meta: pd.Series,\n",
    "    session_id: uuid.UUID,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Insère les métriques globales AHI / AI / HI / ODI / LMI / PLMI\n",
    "    dans metric_summary, à partir du metadata PANDORE.\n",
    "    \"\"\"\n",
    "    metric_cols = [\"ahi\", \"ai\", \"hi\", \"odi\", \"lmi\", \"plmi\"]\n",
    "    nb_inserted = 0\n",
    "\n",
    "    with db_cursor() as cur:\n",
    "        for col in metric_cols:\n",
    "            col_in_meta = None\n",
    "            for c in row_meta.index:\n",
    "                if c.lower() == col:\n",
    "                    col_in_meta = c\n",
    "                    break\n",
    "\n",
    "            if not col_in_meta:\n",
    "                logger.warning(\"Colonne métrique '%s' absente du metadata\", col)\n",
    "                continue\n",
    "\n",
    "            val_str = row_meta[col_in_meta]\n",
    "            if val_str is None or str(val_str).strip() == \"\":\n",
    "                logger.warning(\n",
    "                    \"Valeur vide pour la métrique '%s' dans le metadata\", col\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                val = float(str(val_str).replace(\",\", \".\"))\n",
    "            except Exception:\n",
    "                logger.warning(\n",
    "                    \"Impossible de convertir la métrique '%s' (valeur=%s)\",\n",
    "                    col,\n",
    "                    val_str,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            metric_id = uuid.uuid4()\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO metric_summary (\n",
    "                    metric_id, session_id,\n",
    "                    channel_id, scope_level,\n",
    "                    sleep_stage_code, metric_name,\n",
    "                    metric_value, unit,\n",
    "                    method, window_sec, computed_ts\n",
    "                )\n",
    "                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW())\n",
    "                \"\"\",\n",
    "                (\n",
    "                    str(metric_id),\n",
    "                    str(session_id),\n",
    "                    None,\n",
    "                    \"session\",\n",
    "                    None,\n",
    "                    col.upper(),\n",
    "                    val,\n",
    "                    \"index_per_hour\",\n",
    "                    \"PANDORE_METADATA_V1\",\n",
    "                    None,\n",
    "                ),\n",
    "            )\n",
    "            nb_inserted += 1\n",
    "\n",
    "    logger.info(\n",
    "        \"%d métrique(s) insérée(s) dans metric_summary pour session_id=%s\",\n",
    "        nb_inserted,\n",
    "        session_id,\n",
    "    )\n",
    "    return nb_inserted\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Orchestration : run_etl_s0001 + contrôles\n",
    "# ============================================================\n",
    "\n",
    "def run_etl_s0001():\n",
    "    \"\"\"\n",
    "    Orchestrateur V0 :\n",
    "        - Metadata → subject + study_session + metric_summary\n",
    "        - EDF → data_file + signal_channel\n",
    "        - Hypnogram CSV → hypnogram_epoch + event_annotation\n",
    "    pour la session PANDORE_SAS_DATASET_S0001.\n",
    "    \"\"\"\n",
    "    logger.info(\"=== ETL V0 S0001 démarré ===\")\n",
    "\n",
    "    df_meta = load_metadata_dataframe()\n",
    "    df_hypno = load_hypnogram_df()\n",
    "    row_meta = get_metadata_row_for_s0001(df_meta)\n",
    "\n",
    "    with conn:\n",
    "        subject_id, nb_subject = etl_subject_from_metadata(row_meta)\n",
    "\n",
    "        session_id, nb_session, session_start_utc = etl_study_session_from_metadata(\n",
    "            row_meta, subject_id, df_hypno\n",
    "        )\n",
    "\n",
    "        raw = load_edf()\n",
    "        file_id, nb_df = etl_data_file_from_edf(raw, session_id)\n",
    "        nb_channels = etl_signal_channels_from_edf(raw, file_id)\n",
    "\n",
    "        nb_epochs, nb_events = etl_hypnogram_to_epochs_and_events(\n",
    "            df_hypno, session_id, session_start_utc\n",
    "        )\n",
    "\n",
    "        nb_metrics = etl_metric_summary_from_metadata(row_meta, session_id)\n",
    "\n",
    "    logger.info(\n",
    "        \"=== ETL S0001 terminé : \"\n",
    "        \"subject=%d, session=%d, data_file=%d, \"\n",
    "        \"channels=%d, epochs=%d, events=%d, metrics=%d ===\",\n",
    "        nb_subject,\n",
    "        nb_session,\n",
    "        nb_df,\n",
    "        nb_channels,\n",
    "        nb_epochs,\n",
    "        nb_events,\n",
    "        nb_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "def log_table_count(table_name: str):\n",
    "    \"\"\"\n",
    "    Log le nombre total de lignes dans une table.\n",
    "    \"\"\"\n",
    "    with db_cursor() as cur:\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        nb = cur.fetchone()[0]\n",
    "    logger.info(\"Table %s : %d ligne(s) au total\", table_name, nb)\n",
    "\n",
    "\n",
    "def run_checks_after_etl():\n",
    "    \"\"\"\n",
    "    Lance l'ETL S0001 puis affiche quelques stats / échantillons.\n",
    "    \"\"\"\n",
    "    run_etl_s0001()\n",
    "\n",
    "    for tbl in [\n",
    "        \"subject\",\n",
    "        \"study_session\",\n",
    "        \"data_file\",\n",
    "        \"signal_channel\",\n",
    "        \"hypnogram_epoch\",\n",
    "        \"event_annotation\",\n",
    "        \"metric_summary\",\n",
    "    ]:\n",
    "        log_table_count(tbl)\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\"SELECT * FROM subject LIMIT 5\")\n",
    "        logger.info(\"Sample subject : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\"SELECT * FROM study_session LIMIT 5\")\n",
    "        logger.info(\"Sample study_session : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\"SELECT file_id, file_name FROM data_file LIMIT 5\")\n",
    "        logger.info(\"Sample data_file : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT channel_index, label\n",
    "            FROM signal_channel\n",
    "            ORDER BY channel_index\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(\"Sample signal_channel (10 premiers) : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT epoch_index, start_sec, duration_sec, sleep_stage_code\n",
    "            FROM hypnogram_epoch\n",
    "            ORDER BY epoch_index\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(\"Sample hypnogram_epoch (10 premiers) : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT event_label, onset_sec, duration_sec\n",
    "            FROM event_annotation\n",
    "            ORDER BY onset_sec\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(\"Sample event_annotation (10 premiers) : %s\", cur.fetchall())\n",
    "\n",
    "    with db_cursor(dict_cursor=True) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT metric_name, metric_value\n",
    "            FROM metric_summary\n",
    "            ORDER BY metric_name\n",
    "            LIMIT 10\n",
    "            \"\"\"\n",
    "        )\n",
    "        logger.info(\"Sample metric_summary (10 premiers) : %s\", cur.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_checks_after_etl()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_sleeplab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc182b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLes cellules en cours d’exécution avec .venv_sleeplab (Python 3.13.5) nécessitent le package ipykernel.\n",
      "\u001b[1;31mInstallez « ipykernel » dans l’environnement Python. \n",
      "\u001b[1;31mCommande : « /workspaces/SleepLab_new/.venv_sleeplab/bin/python -m pip install ipykernel -U --force-reinstall »"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0004_OUTPUT_YASA_INTEGRATION.ipynb\n",
    "# ETL des prédictions YASA vers les tables AI\n",
    "# ============================================================\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 1. Logging\n",
    "# ------------------------------------------------------------\n",
    "logger = logging.getLogger(\"sleeplab_etl_yasa\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    )\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 2. Config chemins & Postgres\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR = Path(\"/workspaces/SleepLab_new\").resolve()\n",
    "MODELS_OUTPUT_DIR = BASE_DIR / \"data\" / \"raw\" / \"MODELS_OUTPUT\"\n",
    "\n",
    "# Exemple de fichier (à adapter si besoin)\n",
    "DEFAULT_YASA_JSON = MODELS_OUTPUT_DIR / \"predictions_yasa_S0001.json\"\n",
    "\n",
    "DB_DSN = os.getenv(\n",
    "    \"SLEEPLAB_DB_DSN\",\n",
    "    \"postgresql://sleeplab:sleeplab@postgres:5432/sleeplab\",\n",
    ")\n",
    "\n",
    "logger.info(\"DB_DSN utilisé : %s\", DB_DSN)\n",
    "\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 3. Helpers Postgres locaux\n",
    "# ------------------------------------------------------------\n",
    "def get_conn():\n",
    "    \"\"\"\n",
    "    Retourne une connexion Postgres avec search_path fixé sur 'sleeplab'.\n",
    "    On ouvre une connexion par notebook, simple pour l'instant.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(DB_DSN)\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SET search_path TO sleeplab\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "def db_cursor(conn, dict_cursor: bool = False):\n",
    "    \"\"\"\n",
    "    Helper pour obtenir un curseur.\n",
    "    - dict_cursor=True → RealDictCursor\n",
    "    \"\"\"\n",
    "    cursor_factory = RealDictCursor if dict_cursor else None\n",
    "    return conn.cursor(cursor_factory=cursor_factory)\n",
    "\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 4. Helpers métier : mapping session / run\n",
    "# ------------------------------------------------------------\n",
    "def require_file(path: Path, label: str) -> None:\n",
    "    if not path.exists():\n",
    "        logger.error(\"[FICHIER MANQUANT] %s : %s\", label, path)\n",
    "        raise FileNotFoundError(f\"{label} non trouvé : {path}\")\n",
    "    if not path.is_file():\n",
    "        logger.error(\"[CHEMIN NON FICHIER] %s : %s\", label, path)\n",
    "        raise FileNotFoundError(f\"{label} n'est pas un fichier : {path}\")\n",
    "    logger.info(\"[OK] %s trouvé : %s\", label, path)\n",
    "\n",
    "\n",
    "def extract_subject_suffix_from_filename(json_path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Ex: 'predictions_yasa_S0001.json' -> 'S0001'\n",
    "    (utile pour contrôle/cohérence, mais la vraie source est le nom EDF).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"_S(\\d{4})\\.json$\", json_path.name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        logger.warning(\n",
    "            \"Impossible d'extraire un suffixe 'Sdddd' depuis le nom de fichier %s\",\n",
    "            json_path.name,\n",
    "        )\n",
    "        return None\n",
    "    suffix = f\"S{m.group(1)}\"\n",
    "    logger.info(\"Suffixe subject/session détecté à partir du nom de fichier : %s\", suffix)\n",
    "    return suffix\n",
    "\n",
    "\n",
    "def extract_session_code_from_edf_key(edf_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Exemple :\n",
    "      'PANDORE_SAS_DATASET_S0001_PSG.edf' → 'PANDORE_SAS_DATASET_S0001'\n",
    "    \"\"\"\n",
    "    m = re.match(r\"^(?P<session_root>.+_S\\d{4})_PSG\\.edf$\", edf_key)\n",
    "    if not m:\n",
    "        raise ValueError(\n",
    "            f\"Impossible d'extraire session_root depuis la clé EDF '{edf_key}'\"\n",
    "        )\n",
    "    return m.group(\"session_root\")\n",
    "\n",
    "\n",
    "def get_session_id_from_code(conn, session_code: str) -> str:\n",
    "    \"\"\"\n",
    "    SELECT session_id FROM study_session WHERE session_code = %s\n",
    "    \"\"\"\n",
    "    with db_cursor(conn, dict_cursor=True) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT session_id\n",
    "            FROM study_session\n",
    "            WHERE session_code = %s\n",
    "            \"\"\",\n",
    "            (session_code,),\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        if not row:\n",
    "            raise RuntimeError(\n",
    "                f\"Aucune session trouvée pour session_code={session_code}\"\n",
    "            )\n",
    "        return row[\"session_id\"]\n",
    "\n",
    "\n",
    "def get_or_create_ai_model_run(\n",
    "    conn,\n",
    "    session_id: str,\n",
    "    model_name: str,\n",
    "    model_version: Optional[str],\n",
    "    training_tag: Optional[str],\n",
    "    source_file_name: str,\n",
    "    params_json: Optional[Dict[str, Any]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Crée ou récupère un run d'IA.\n",
    "\n",
    "    Hypothèse : on a une contrainte d'unicité (session_id, model_name, model_version, source_file_name)\n",
    "    ou équivalent dans le script SQL d'amendement.\n",
    "    \"\"\"\n",
    "    version_key = model_version or \"\"\n",
    "    with db_cursor(conn) as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT run_id\n",
    "            FROM ai_model_run\n",
    "            WHERE session_id = %s\n",
    "              AND model_name = %s\n",
    "              AND COALESCE(model_version,'') = %s\n",
    "              AND source_file_name = %s\n",
    "            \"\"\",\n",
    "            (session_id, model_name, version_key, source_file_name),\n",
    "        )\n",
    "        row = cur.fetchone()\n",
    "        if row:\n",
    "            run_id = row[0]\n",
    "            logger.info(\n",
    "                \"Run AI déjà présent (session_id=%s, model=%s, version=%s, src=%s) → run_id=%s\",\n",
    "                session_id,\n",
    "                model_name,\n",
    "                model_version,\n",
    "                source_file_name,\n",
    "                run_id,\n",
    "            )\n",
    "            return run_id\n",
    "\n",
    "        run_id = str(uuid.uuid4())\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO ai_model_run (\n",
    "                run_id, session_id,\n",
    "                model_name, model_version,\n",
    "                training_tag, source_file_name,\n",
    "                params_json, created_ts\n",
    "            )\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s, NOW())\n",
    "            \"\"\",\n",
    "            (\n",
    "                run_id,\n",
    "                session_id,\n",
    "                model_name,\n",
    "                model_version,\n",
    "                training_tag,\n",
    "                source_file_name,\n",
    "                json.dumps(params_json) if params_json is not None else None,\n",
    "            ),\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Nouveau run AI inséré : run_id=%s (model=%s, version=%s, session_id=%s)\",\n",
    "            run_id,\n",
    "            model_name,\n",
    "            model_version,\n",
    "            session_id,\n",
    "        )\n",
    "        return run_id\n",
    "\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 5. Parsing des prédictions YASA\n",
    "# ------------------------------------------------------------\n",
    "STAGE_ORDER = [\"W\", \"N1\", \"N2\", \"N3\", \"R\"]  # ordre pour l'argmax\n",
    "\n",
    "\n",
    "def parse_yasa_prediction_payload(pred_payload: Any) -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    pred_payload peut être :\n",
    "      - un dict { \"N1\": {\"0\": prob, ...}, \"N2\": {...}, ...}\n",
    "      - OU une string JSON de ce dict.\n",
    "\n",
    "    Retour :\n",
    "      dict[epoch_index] -> dict[stage_code] -> prob\n",
    "      ex: {0: {\"N1\": 0.01, \"N2\": 0.93, ...}, 1: {...}, ...}\n",
    "    \"\"\"\n",
    "    if isinstance(pred_payload, str):\n",
    "        try:\n",
    "            pred_dict = json.loads(pred_payload)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Impossible de parser prediction YASA (string) : {e}\")\n",
    "    elif isinstance(pred_payload, dict):\n",
    "        pred_dict = pred_payload\n",
    "    else:\n",
    "        raise ValueError(f\"Type inattendu pour 'prediction' YASA : {type(pred_payload)}\")\n",
    "\n",
    "    per_epoch: Dict[int, Dict[str, float]] = {}\n",
    "    for stage_label, epoch_map in pred_dict.items():\n",
    "        # On normalise le label en code standard si besoin\n",
    "        stage = stage_label.upper()\n",
    "        if stage == \"N0\":  # au cas où\n",
    "            stage = \"W\"\n",
    "\n",
    "        if stage not in STAGE_ORDER:\n",
    "            # On loggue mais on ne bloque pas\n",
    "            logger.warning(\"Stade inattendu dans YASA : '%s'\", stage_label)\n",
    "            continue\n",
    "\n",
    "        if not isinstance(epoch_map, dict):\n",
    "            logger.warning(\n",
    "                \"epoch_map inattendu pour stage=%s (type=%s)\", stage, type(epoch_map)\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        for epoch_idx_str, prob in epoch_map.items():\n",
    "            try:\n",
    "                epoch_idx = int(epoch_idx_str)\n",
    "            except Exception:\n",
    "                logger.warning(\n",
    "                    \"Index d'epoch non entier dans YASA : key=%s\", epoch_idx_str\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                p = float(prob)\n",
    "            except Exception:\n",
    "                logger.warning(\n",
    "                    \"Probabilité non numérique pour epoch=%s, stage=%s : %s\",\n",
    "                    epoch_idx,\n",
    "                    stage,\n",
    "                    prob,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            per_epoch.setdefault(epoch_idx, {})[stage] = p\n",
    "\n",
    "    return per_epoch\n",
    "\n",
    "\n",
    "def compute_predicted_stage(probs_by_stage: Dict[str, float]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retourne le stage avec la probabilité max dans l'ordre STAGE_ORDER.\n",
    "    Si dict vide → None.\n",
    "    \"\"\"\n",
    "    if not probs_by_stage:\n",
    "        return None\n",
    "    best_stage = None\n",
    "    best_prob = -1.0\n",
    "    for stage in STAGE_ORDER:\n",
    "        p = probs_by_stage.get(stage)\n",
    "        if p is None:\n",
    "            continue\n",
    "        if p > best_prob:\n",
    "            best_prob = p\n",
    "            best_stage = stage\n",
    "    return best_stage\n",
    "\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 6. ETL principal YASA\n",
    "# ------------------------------------------------------------\n",
    "def etl_yasa_from_json_file(\n",
    "    json_path: Path,\n",
    "    model_name: str = \"YASA\",\n",
    "    model_version: Optional[str] = None,\n",
    "    training_tag: Optional[str] = None,\n",
    "    epoch_length_sec: int = 30,\n",
    ") -> Tuple[List[str], int]:\n",
    "    \"\"\"\n",
    "    Intègre un fichier JSON YASA dans ai_model_run + ai_epoch_prediction.\n",
    "\n",
    "    Retourne :\n",
    "      - liste des run_id créés\n",
    "      - nombre total d'epochs insérés\n",
    "    \"\"\"\n",
    "    require_file(json_path, f\"JSON YASA {model_name}\")\n",
    "    suffix = extract_subject_suffix_from_filename(json_path)\n",
    "\n",
    "    logger.info(\"=== ETL YASA démarré pour %s ===\", json_path.name)\n",
    "    raw_text = json_path.read_text()\n",
    "    data = json.loads(raw_text)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(f\"Structure JSON inattendue (top-level) : {type(data)}\")\n",
    "\n",
    "    conn = get_conn()\n",
    "    total_epochs = 0\n",
    "    run_ids: List[str] = []\n",
    "\n",
    "    try:\n",
    "        with conn:\n",
    "            for edf_key, payload in data.items():\n",
    "                logger.info(\"Traitement de la clé EDF : %s\", edf_key)\n",
    "\n",
    "                session_code = extract_session_code_from_edf_key(edf_key)\n",
    "                logger.info(\"session_code dérivé de la clé EDF : %s\", session_code)\n",
    "\n",
    "                if suffix and suffix.lower() not in session_code.lower():\n",
    "                    logger.warning(\n",
    "                        \"Le suffixe '%s' (depuis le nom du fichier JSON) \"\n",
    "                        \"ne correspond pas au session_code '%s' (clé EDF).\",\n",
    "                        suffix,\n",
    "                        session_code,\n",
    "                    )\n",
    "\n",
    "                session_id = get_session_id_from_code(conn, session_code)\n",
    "\n",
    "                # payload attendu : dict avec au moins 'prediction'\n",
    "                if not isinstance(payload, dict):\n",
    "                    logger.warning(\n",
    "                        \"Payload inattendu pour %s (type=%s) → ignoré\",\n",
    "                        edf_key,\n",
    "                        type(payload),\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                pred_payload = payload.get(\"prediction\")\n",
    "                if pred_payload is None:\n",
    "                    logger.warning(\n",
    "                        \"Aucun champ 'prediction' pour %s → ignoré\", edf_key\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                per_epoch = parse_yasa_prediction_payload(pred_payload)\n",
    "                logger.info(\n",
    "                    \"YASA : %d epochs détectés pour session_code=%s\",\n",
    "                    len(per_epoch),\n",
    "                    session_code,\n",
    "                )\n",
    "\n",
    "                # Création du run\n",
    "                params_json = {\n",
    "                    \"source_edf_key\": edf_key,\n",
    "                    \"notes\": \"YASA predictions imported from JSON\",\n",
    "                }\n",
    "                run_id = get_or_create_ai_model_run(\n",
    "                    conn,\n",
    "                    session_id=session_id,\n",
    "                    model_name=model_name,\n",
    "                    model_version=model_version,\n",
    "                    training_tag=training_tag,\n",
    "                    source_file_name=json_path.name,\n",
    "                    params_json=params_json,\n",
    "                )\n",
    "                run_ids.append(run_id)\n",
    "\n",
    "                # Nettoyage préalable : on supprime les prédictions existantes de ce run\n",
    "                with db_cursor(conn) as cur:\n",
    "                    cur.execute(\n",
    "                        \"DELETE FROM ai_epoch_prediction WHERE run_id = %s\",\n",
    "                        (run_id,),\n",
    "                    )\n",
    "                    nb_deleted = cur.rowcount\n",
    "                if nb_deleted:\n",
    "                    logger.info(\n",
    "                        \"%d prédiction(s) existante(s) supprimée(s) pour run_id=%s\",\n",
    "                        nb_deleted,\n",
    "                        run_id,\n",
    "                    )\n",
    "\n",
    "                # Insertion des epochs\n",
    "                inserted = 0\n",
    "                with db_cursor(conn) as cur:\n",
    "                    for epoch_idx in sorted(per_epoch.keys()):\n",
    "                        probs = per_epoch[epoch_idx]\n",
    "                        pred_stage = compute_predicted_stage(probs)\n",
    "\n",
    "                        onset_sec = float(epoch_idx * epoch_length_sec)\n",
    "                        duration_sec = float(epoch_length_sec)\n",
    "\n",
    "                        extra_json = {\n",
    "                            \"all_probs\": probs,\n",
    "                            \"epoch_index_source\": \"YASA\",\n",
    "                        }\n",
    "\n",
    "                        # On mappe sur les colonnes prob_w, prob_n1, etc.\n",
    "                        prob_w = probs.get(\"W\")\n",
    "                        prob_n1 = probs.get(\"N1\")\n",
    "                        prob_n2 = probs.get(\"N2\")\n",
    "                        prob_n3 = probs.get(\"N3\")\n",
    "                        prob_r = probs.get(\"R\")\n",
    "\n",
    "                        epoch_pred_id = str(uuid.uuid4())\n",
    "                        cur.execute(\n",
    "                            \"\"\"\n",
    "                            INSERT INTO ai_epoch_prediction (\n",
    "                                epoch_pred_id, run_id,\n",
    "                                epoch_index, onset_sec, duration_sec,\n",
    "                                predicted_stage_code,\n",
    "                                prob_w, prob_n1, prob_n2, prob_n3, prob_r,\n",
    "                                extra_json\n",
    "                            )\n",
    "                            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "                            \"\"\",\n",
    "                            (\n",
    "                                epoch_pred_id,\n",
    "                                run_id,\n",
    "                                int(epoch_idx),\n",
    "                                onset_sec,\n",
    "                                duration_sec,\n",
    "                                pred_stage,\n",
    "                                prob_w,\n",
    "                                prob_n1,\n",
    "                                prob_n2,\n",
    "                                prob_n3,\n",
    "                                prob_r,\n",
    "                                json.dumps(extra_json),\n",
    "                            ),\n",
    "                        )\n",
    "                        inserted += 1\n",
    "\n",
    "                logger.info(\n",
    "                    \"YASA : %d prédiction(s) epoch insérées pour run_id=%s\",\n",
    "                    inserted,\n",
    "                    run_id,\n",
    "                )\n",
    "                total_epochs += inserted\n",
    "\n",
    "        logger.info(\n",
    "            \"=== ETL YASA terminé pour %s : runs=%d, epochs=%d ===\",\n",
    "            json_path.name,\n",
    "            len(run_ids),\n",
    "            total_epochs,\n",
    "        )\n",
    "        return run_ids, total_epochs\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------\n",
    "# 7. Fonction de contrôle : run_checks_after_etl_yasa\n",
    "# ------------------------------------------------------------\n",
    "def log_table_count(conn, table: str):\n",
    "    with db_cursor(conn) as cur:\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        nb = cur.fetchone()[0]\n",
    "    logger.info(\"Table %s : %d ligne(s)\", table, nb)\n",
    "\n",
    "\n",
    "def run_checks_after_etl_yasa(\n",
    "    json_path: Path,\n",
    "    model_version: Optional[str] = None,\n",
    "    training_tag: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Lance l'ETL YASA puis affiche quelques stats / échantillons.\n",
    "    \"\"\"\n",
    "    run_ids, total_epochs = etl_yasa_from_json_file(\n",
    "        json_path=json_path,\n",
    "        model_version=model_version,\n",
    "        training_tag=training_tag,\n",
    "    )\n",
    "\n",
    "    conn = get_conn()\n",
    "    try:\n",
    "        logger.info(\"YASA : %d run(s) créés/mis à jour, %d epochs insérés\", len(run_ids), total_epochs)\n",
    "\n",
    "        # Compteurs globaux AI\n",
    "        for tbl in [\"ai_model_run\", \"ai_epoch_prediction\"]:\n",
    "            log_table_count(conn, tbl)\n",
    "\n",
    "        # Samples pour le dernier run\n",
    "        if run_ids:\n",
    "            last_run = run_ids[-1]\n",
    "            with db_cursor(conn, dict_cursor=True) as cur:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT *\n",
    "                    FROM ai_model_run\n",
    "                    WHERE run_id = %s\n",
    "                    \"\"\",\n",
    "                    (last_run,),\n",
    "                )\n",
    "                logger.info(\"Sample ai_model_run(last_run) : %s\", cur.fetchall())\n",
    "\n",
    "            with db_cursor(conn, dict_cursor=True) as cur:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT epoch_index, onset_sec, duration_sec,\n",
    "                           predicted_stage_code, prob_w, prob_n1, prob_n2, prob_n3, prob_r\n",
    "                    FROM ai_epoch_prediction\n",
    "                    WHERE run_id = %s\n",
    "                    ORDER BY epoch_index\n",
    "                    LIMIT 10\n",
    "                    \"\"\",\n",
    "                    (last_run,),\n",
    "                )\n",
    "                logger.info(\n",
    "                    \"Sample ai_epoch_prediction(last_run, 10 premiers) : %s\",\n",
    "                    cur.fetchall(),\n",
    "                )\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Exemple d'appel dans le notebook :\n",
    "from pathlib import Path\n",
    "yasa_json = MODELS_OUTPUT_DIR / \"predictions_yasa_S0001.json\"\n",
    "run_checks_after_etl_yasa(json_path=yasa_json, model_version=\"v1\", training_tag=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_sleeplab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
